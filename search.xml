<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>RNN+LSTM</title>
      <link href="/Algorithm/RNN-LSTM/"/>
      <url>/Algorithm/RNN-LSTM/</url>
      
        <content type="html"><![CDATA[<h2 id="RNN-LSTM"><a href="#RNN-LSTM" class="headerlink" title="RNN+LSTM"></a>RNN+LSTM</h2><a id="more"></a><ul><li><h4 id="Why-we-need-RNN"><a href="#Why-we-need-RNN" class="headerlink" title="Why we need RNN?"></a>Why we need RNN?</h4><p>​    普通的神经网络虽然强大，但是只能单独的去处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理<strong>序列</strong>的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。</p><p>以nlp的一个最简单词性标注任务来说，将我 吃 苹果 三个单词标注词性为 我/nn 吃/v 苹果/nn。</p><p>这个任务的输入是：我 吃 苹果 （已经分词好的句子）</p><p>这个任务的输出是：我/nn 吃/v 苹果/nn  (词性标注好的句子)</p><p>对于这个任务来说，我们当然可以直接用普通的神经网络来做，给网络的训练数据格式了就是我-&gt; 我/nn 这样的多个单独的单词-&gt;词性标注好的单词。</p><p><strong>但是很明显，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测苹果的时候，由于前面的吃是一个动词，那么很显然苹果作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。</strong></p><p>所以为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。</p></li><li><h4 id="RNN的结构"><a href="#RNN的结构" class="headerlink" title="RNN的结构"></a>RNN的结构</h4><p>循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。</p><p><img src="/Algorithm/RNN-LSTM/v2-206db7ba9d32a80ff56b6cc988a62440_r.jpg" alt=""></p></li><li><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>​    RNN功能这么强大，那么LSTM又是干什么的呢？</p><p>​    RNN 是在有顺序的数据上进行学习的. 为了记住这些数据, RNN 会像人一样产生对先前发生事件的记忆。</p><p>​    想像现在有这样一个 RNN, 他的输入值是一句话: “我今天要去北京旅游, 首先我要坐大巴去机场，然后登上飞机，经过漫长的旅途，就到站啦。”那么用 RNN 来分析, 我今天去了什么地方呢？ RNN可能会给出“菏泽”这个答案。因为结论出错, RNN就要开始学习这么长一段话和 “北京“的关系 , 而RNN需要的关键信息 ”北京”却出现在句子开头。</p><p>​    ”北京“这个信息的记忆要进过长途跋涉才能抵达最后一个时间点，然后我们得到误差。而且在反向传递得到的误差的时候，他在每一步都会乘以一个自己的参数 W。如果这个 W 是一个小于1的数, 比如0.9。这个0.9不断乘以误差, 误差传到初始时间点也会是一个接近于零的数, 所以对于初始时刻, 误差相当于就消失了。我们把这个问题叫做梯度消失或者梯度弥散。</p><p><img src="/Algorithm/RNN-LSTM/lstm3.png" alt="梯度弥散"></p><p>反之如果 W 是一个大于1的数，比如1.1。不断累乘，则到最后变成了无穷大的数，RNN被这无穷大的数撑死了, 这种情况我们叫做梯度爆炸。 这就是普通 RNN 没有办法回忆起久远记忆的原因。</p><p><img src="/Algorithm/RNN-LSTM/lstm4.png" alt="梯度爆炸"></p><p>​    LSTM 就是为了解决这个问题而诞生的。LSTM 和普通 RNN 相比，多出了三个控制器(输入控制，输出控制，忘记控制)。现在, LSTM内部的情况是这样：</p><p><img src="/Algorithm/RNN-LSTM/lstm5.png" alt=""></p><p>​    多了一个控制全局的记忆, 我们用粗线代替.。输入控制，输出控制，忘记控制三个控制器都是在原始的 RNN 体系上, 我们先看输入方面 , 如果此时的分线剧情对于剧终结果十分重要, 输入控制就会将这个分线剧情按重要程度写入主线剧情进行分析. 再看忘记方面, 如果此时的分线剧情更改了我们对之前剧情的想法, 那么忘记控制就会将之前的某些主线剧情忘记, 按比例替换成现在的新剧情。所以主线剧情的更新就取决于输入和忘记 控制。最后的输出方面, 输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么。基于这些控制机制, LSTM就可以带来更好的结果。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CNN</title>
      <link href="/Algorithm/CNN/"/>
      <url>/Algorithm/CNN/</url>
      
        <content type="html"><![CDATA[<h2 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h2><a id="more"></a><h4 id="一、我们需要CNN"><a href="#一、我们需要CNN" class="headerlink" title="一、我们需要CNN"></a>一、我们需要CNN</h4><p>​    在全连接神经网络中，每相邻两层之间的每个神经元之间都是有边相连的。当输入层的特征维度变得很高时，这时全连接网络需要训练的参数就会增大很多，计算速度就会变得很慢，例如一张黑白的 28×28 的手写数字图片，输入层的神经元就有784个，如下图所示：<img src="/Algorithm/CNN/2708793-9bf202a891b30d47.png" alt=""></p><p>​    若在中间只使用一层15个神经元的隐藏层，那么参数 w 就有 784 × 15 = 11760 多个；若输入的是 28×28 带有颜色的RGB格式的手写数字图片，那么参数 w 的个数还需要再乘以 3。这很容易看出使用全连接神经网络处理图像中的需要训练参数过多的问题。</p><p>​    而在卷积神经网络（Convolutional Neural Network,CNN）中，卷积层的神经元只与前一层的部分神经元节点相连，即它的神经元间的连接是非全连接的，且同一层中某些神经元之间的连接的权重 w 和偏移 b 是共享的（Shared Weights），这样大量地减少了需要训练参数的数量。</p><h4 id="二、CNN的卷积层"><a href="#二、CNN的卷积层" class="headerlink" title="二、CNN的卷积层"></a>二、CNN的卷积层</h4><p>在卷积层中有几个重要的概念：<br> <strong>1、Local receptive fields（感受野）</strong><br>     假设输入的是一个 28×28 的的二维神经元，我们定义5×5 的 一个 Local receptive fields（感受野），即隐藏层的神经元与输入层的 5×5 个神经元相连，这个 5×5 的区域就称之为 Local Receptive Fields，如下图所示：</p><p>​                                                     <img src="/Algorithm/CNN/2708793-4049d62e332d5343.png" alt=""></p><p><strong>2、Shared weights（共享权值）</strong><br>     隐含层的每一个神经元都连接 5x5个图像区域，也就是说每一个神经元存在 25个连接权值。也就是说下一层的每个神经元如果用的是同一个卷积核去卷积图像。这样我们就只有 25 个参数啊！不管你隐层的神经元个数有多少，两层间的连接我只有 25 个参数啊！这就是卷积神经网络的主打卖点。</p><p>​                                           <img src="/Algorithm/CNN/2708793-e10ead4323c94e00.png" alt=""></p><p>​    <strong>但是</strong>，你就会想，这样提取特征也忒不靠谱吧，这样你只提取了一种特征啊？我们需要提取多种特征啊！假如一种滤波器只提取图像的一种特征，那么我们需要提取不同的特征，怎么办，加多几种滤波器不就行了吗？所以假设我们加到 100 种滤波器，每种滤波器的参数不一样，表示它提出输入图像的不同特征。这样每种滤波器去卷积图像就得到对图像的不同特征的放映，我们称之为Feature Map。所以 100 种卷积核就有 100 个 Feature Map。这 100 个 Feature Map 就组成了一层神经元。所以我们这一层有多少个参数了？100 种卷积核 x 每种卷积核共享 25 个参数= 100 x 25 = 2500，也就是2500个参数。</p><p><img src="/Algorithm/CNN/2708793-38ab7530eb04044b.png" alt=""></p><pre><code> 需要注意的一点是，上面的讨论都没有考虑每个神经元的偏置部分。所以权值个数需要加1 。这个也是同一种滤波器共享的。因此在CNN的卷积层，我们需要训练的参数大大地减少到了 (5×5+1)×100=2600个。</code></pre><p><img src="/Algorithm/CNN/2708793-e03633a7da7da8cd.png" alt=""></p><h4 id="三、“卷积”操作"><a href="#三、“卷积”操作" class="headerlink" title="三、“卷积”操作"></a>三、“卷积”操作</h4><p>​    当给一张新的图时，CNN并不能准确地知道这些 Features 到底要匹配原图的哪些部分，所以它会在原图中每一个可能的位置进行尝试。这样在原始整幅图上每一个位置进行匹配计算。这个我们用来匹配的过程就被称为卷积操作，这也就是卷积神经网络名字的由来。</p><p>这个卷积操作背后的数学知识其实非常的简单。要计算一个滤波器和其在原图上对应的某一小块的结果，只需要简单地将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可。</p><p><img src="/Algorithm/CNN/3206659-74277c15fb33e2e6.jpg" alt=""></p><p>​    我们以上图为例，卷积神经元的output是如何计算出来的。蓝色区域和红色区域对应的数字进行相乘然后再求和，最后和Bias相加，得到对应的数字，依次存放到绿色矩阵里面。第一个矩阵结果是3， 第二个矩阵的结果是2，第三个矩阵的结果是0，所以3 + 2 + 0 = 5，最后加上第一个Bias参数1，所以第一个3<em>3</em>3的区域神经输出结果是 6。</p><p>​    然后依次进行。</p><h4 id="四、池化-Pooling"><a href="#四、池化-Pooling" class="headerlink" title="四、池化(Pooling)"></a>四、池化(Pooling)</h4><p>​    当输入经过卷积层时，若感受野比较小，步长stride比较小，得到的feature map（特征图）还是比较大，可以通过池化层来对每一个 feature map 进行降维操作，输出的深度还是不变的，依然为 feature map 的个数。</p><p>​    池化可以将一幅大的图像缩小，同时又保留其中的重要信息。池化背后的数学顶多也就是小学二年级水平。它就是将输入图像进行缩小，减少像素信息，只保留重要信息。通常情况下，池化都是 2×2 大小，比如对于 max-pooling 来说，就是取输入图像中 2×2 大小的块中的最大值，作为结果的像素值，相当于将原始图像缩小了 4 倍。(注：同理，对于average-pooling来说，就是取 2×2 大小块的平均值作为结果的像素值。)</p><p>​    所以，池化的实质就是把图片变模糊了，相当于给图片打了马赛克，如下图：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2708793-47fdbcd5f6b43d0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/494/format/webp" alt="img"></p><p>通过加入池化层，可以很大程度上减少计算量，降低机器负载。</p><p>到现在为止，已经讲了从输入到池化的过程，图片也从原来的 28 × 28 变成了 12 × 12，如下图</p><p>​                                      <img src="/Algorithm/CNN/2708793-a7a268b6c29571bf.png" alt=""></p><h4 id="五、激活函数Relu-Rectified-Linear-Units"><a href="#五、激活函数Relu-Rectified-Linear-Units" class="headerlink" title="五、激活函数Relu (Rectified Linear Units)"></a>五、激活函数Relu (Rectified Linear Units)</h4><p>这里，我们用Relu作为激活函数：</p><p><img src="/Algorithm/CNN/2708793-e0374cbc4b1421a9.png" alt=""></p><h4 id="六、全连接层-Fully-connected-layers"><a href="#六、全连接层-Fully-connected-layers" class="headerlink" title="六、全连接层(Fully connected layers)"></a>六、全连接层(Fully connected layers)</h4><p>​    卷积神经网络的目的就是降低网络复杂度，减少权值W和偏置值b的个数，它不是一个完整的网络，所以，在最后还须加入普通的神经网络（全连接层）。</p><p><img src="/Algorithm/CNN/2708793-9a81462bcdc5e270.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>南巡，难寻——黄山</title>
      <link href="/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E9%BB%84%E5%B1%B1/"/>
      <url>/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E9%BB%84%E5%B1%B1/</url>
      
        <content type="html"><![CDATA[<h2 id="南巡，难寻——黄山"><a href="#南巡，难寻——黄山" class="headerlink" title="南巡，难寻——黄山"></a>南巡，难寻——黄山</h2><a id="more"></a><p>​    岳归否山，黄归否岳。</p><p>​    南巡之行已至终程，景色无边，钱袋空虚，行囊也愈发沉重。将背包寄存在山下，买过票后（票是真滴贵），坐上登山汽车。远远看去，山势蜿蜒起伏，其色绿褐交织，幻想着奇松怪石，险涧飞瀑。心仪已久的黄山美人儿终于要慢慢揭开她的面纱，内心激荡翻滚。<img src="/travel/南巡，难寻——黄山/80347b70278d04021c5b011abf95a84.jpg" alt=""></p><p>​    望山跑死马，正当我担心路上是否有加油站时，汽车终于到了。不是假期的周末，依旧人流漫山。车停在慈光阁处（我们选择从前山上山，最后了解到真相唏嘘不已），身处黄山山脉中，不识东西南北，更不见黄山真相。</p><p>缆车处排了好长的队，我不愿等，于是选择步行上山，顺便可以观路途风景。（步行上山是我做过的最智障累成狗的决定）<img src="/travel/南巡，难寻——黄山/2c6888057a5bd5caecf6ded0319ac8a.jpg" alt=""></p><p>​    从慈光阁一路爬行，期间经过了像立马亭这样的无数个小站，现在看来，立马亭这三个字甚是嘲讽，当时就应驻足返程，坐上缆车，一路轻松愉快。当我四条腿已经支撑不住的时候，终于来到了下一个大站——缆车的终点，玉屏峰。    用一种你们错过了许多景色的神情走过从缆车上下来的人，酸麻的手脚竟然有了昂扬自得的欢呼雀跃感。抬头一望便看到了迎客松。<img src="/travel/南巡，难寻——黄山/82d6a83fed1a7c2a17a811aad5d436c.jpg" alt=""></p><p>​    腰杆挺直，手舞足蹈，又置身云雾中，像是接引游客去天界的使者。咦？什么时候有的云雾呢？我揉眼再次看去，一切又不一样了：天都峰上刻登峰造极，其险峻如天国之门，其旁的犁云峰上，一只松鼠两腿弯曲，蓄势待发，正要跳入天都；而右侧，山峰如湖中白莲，随风而动，似要飘至眼前。我晃晃脑袋，使意识恢复清醒，眼前玉屏矗立，远处海天相接。黄山最高峰——莲花峰，如海中孤岛。爬山的辛苦，让我下定决心一定不去莲花峰！</p><p>​    沿山路继续前行，下一个目标是光明顶，飞来石，过百步云梯，一线天，凭借着一股想看到飞来石的意志，我终于登上了莲花峰！没错，我迷路了！没错，我登上了最高峰！一股豪气自心底升起是不存在的，如果可以，我想纵身而下，那么这云海一定是柔软的席梦思床垫，青松为枕，云海做铺，让我好好安眠。<img src="/travel/南巡，难寻——黄山/94ec78482fa5aa8eafbe178996f7672.jpg" alt=""></p><p>​    眼见夕阳西下，再去光明顶，从后山下山的愿望已经破灭，于是重返玉屏峰，爬进缆车里，返程。</p><p>​    听别的游客说，前山雄，后山秀，原来，我心念已久的黄山美人儿，是双性，而我我只见到它雄的一面。回程的路上，我一直唉声叹气，光明顶，飞来石没见到，更勿论猴子观海梦笔生花。</p><p>​    后来我想通了，人生难免遗憾嘛，就算做好万全的准备，也不可能观遍所有的风景，给自己留点遗憾，留点念想，留点积极向上努力奋斗下次再来的动力，多好。嗯，近期不想爬山了，如果多年后再来黄山，我只有一个要求，请让我！坐缆车！</p>]]></content>
      
      
      <categories>
          
          <category> travel </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>南巡，难寻——宏村</title>
      <link href="/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E5%AE%8F%E6%9D%91/"/>
      <url>/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E5%AE%8F%E6%9D%91/</url>
      
        <content type="html"><![CDATA[<h2 id="南巡，难寻——宏村"><a href="#南巡，难寻——宏村" class="headerlink" title="南巡，难寻——宏村"></a>南巡，难寻——宏村</h2><a id="more"></a><p>​    一生痴绝处，无梦到徽州。</p><p>​    辞别南京扬州，从合肥换乘，于4月11日中午抵达黄山市。坐上旅游大巴前往宏村，山间公路状如羊肠，蜿蜒曲折，途中深涧沟壑，陡坡急弯，惊慌中抓住座椅不敢妄动。将头偏向窗外，便是成片的梯田，油菜花花期已过，一片绿汪汪里点缀着零星黄色，更远处层峦叠嶂，云吞雾隐，莫非已至群玉山中？途中经过了齐云山和西递，未作停留，大约一个多小时的山路，便到了宏村。</p><p>​    青山横北郭，白水绕东城。宏村被群山包围着，像是妙龄女子同时获得了几个强壮男人的青睐。下车便是民宿老板迎至门外，从我手中接过行李，一路指引着到达民宿。入的村内，便见到了传说中的宏村南湖。许是刚下过大雨的缘故，湖水略显浑浊。湖南岸倩影排座，画板斑斓，远看去，画板中画与现实之境相得益彰，恍惚间，自己好似也置身画中，又是谁在用画笔勾勒这山水和我这风度翩翩的伟男子呢？<img src="/travel/南巡，难寻——宏村/18b544fe7b2171f391f30623ba2adab.jpg" alt="">    </p><p>​    无边细雨湿春泥，一边吟过画桥西。山云相接，被青灰色的墨汁自下而上刷过一般，阳光透过淡青色的云雾，给水墨宏村镶上一层金边。卧虎藏龙中李慕白牵马过画桥，一心想交出青冥剑，退隐江湖，过安静平和的日子。然人在江湖，身不由己，为恶者，不能以德服之，还需心中之直，宝剑之利。我身着汉服走在青石板街，此时，我又不是舞文弄墨的酸腐儒生，而是浪迹天涯的仗剑侠客，十步杀一人，千里不留行。<img src="/travel/南巡，难寻——宏村/84eb3c6b79cd6f1e8281135587702e6.jpg" alt=""></p><p>·    宏村大多处在平坦地带，靠山脚，依山势而上，其背倚黄山余脉羊栈领、雷岗山等，地势较高，状似牛型。村口的两颗古树是牛角，村周四处小桥是牛蹄，月沼是牛肚，村庄是牛身。村子屋舍俨然，徽商屋内天花板更是彩绘金丝，不过更喜普通人家的古朴陈旧。来到月沼旁得月楼，得月楼，取自近水楼台先得月之意，餐桌上山笋鲜美，腊肉浓香，猴魁清雅，不禁食欲大开。<img src="/travel/南巡，难寻——宏村/8d385c0f06fc5deaaae1742c4b27363.jpg" alt=""></p><p>​    次日清晨，来到村外田野，又是一番清新亮丽之景，在水墨画中，出现了一抹奇色。山脚下的田野上，卧眠的水牛，嚼草的马儿，湿地里，游动的鸭子，山边飘荡的热气球，一片自由祥和。走过水牛骏马热气球，却发现有绳子拴住，连嘎嘎的鸭子，也是用篱笆隔开。是哦，哪有绝对的自由和无拘无束，我身负手铐脚链，眼中全是自由，这便够了。<img src="/travel/南巡，难寻——宏村/dc9e413f6751ce61591197edc6e74be.jpg" alt=""></p><p>​    玉蝶穿花零碎锦，一天飞絮斗轻狂。走在宏村，走入山野，仿佛我500年前也曾在这，许下来生再会。</p>]]></content>
      
      
      <categories>
          
          <category> travel </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>南巡，难寻——扬州</title>
      <link href="/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E6%89%AC%E5%B7%9E/"/>
      <url>/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E6%89%AC%E5%B7%9E/</url>
      
        <content type="html"><![CDATA[<h2 id="南巡，难寻——扬州"><a href="#南巡，难寻——扬州" class="headerlink" title="南巡，难寻——扬州"></a>南巡，难寻——扬州</h2><a id="more"></a><p>​    一入扬州误终生</p><p>​    4月8日晚，一下车站，抬头便看到漆黑的夜幕挂着一轮皎月，月儿如钩，勾人心魄。便是无奈三分明月夜，二分无赖是扬州。入住扬州民宿，民宿内桌球沙发，书籍花园，还有英武不凡的黑白花色大猫，被困笼中，上下腾挪，呜咽不息，余感身体乏倦，便卧床休息。<br>​    4月9日晨，天色灰暗，不见阳光，湿润的空气将泥土花草香气揉捏成团，狠狠的砸在脸上，忽的又散开，无孔不入的乱钻。已是工作日，街上人仍步履缓慢，多见步行或单车，少见四轮驰骋。晃悠至冶春茶社来吃早茶，蟹黄汤包足有拳头般大，吸管插入，鲜亮的汤汁便涌入口中，顿觉香气四溢，食物鲜香与花草芳香在体内混杂交织，又相互争斗，不由得打了个激灵。又吃过五丁包，烧麦，一杯绿杨春入腹，神清气爽赶往瘦西湖。<img src="/travel/南巡，难寻——扬州/23d5f4cc27cecae99a922027a2123af.jpg" alt=""></p><p>​    行至半程，一股水雾扑面而来，紧接着便是细雨纷飞，我突然慌了，手中无伞，只能快步朝瘦西湖赶，抱怨这不适合旅游的天气。途经商店，买过伞，终于幸免于难。购票进入园中，原来瘦西湖并不只是湖而已，而是湖上园林，瘦西湖原是扬州护城河，是京杭大运河的一条支流。后来扬州盐商在河两岸打造园林风景，逐渐出现了二十四景。如果把杭州西湖比作会向瑶台月下逢的唐朝杨贵妃，那么瘦西湖就是汉代体轻能为掌上舞的赵飞燕，轻盈苗条，小巧玲珑。步行至二十四桥，上桥十二阶，下桥十二阶，传说曾有二十四个江南美女立于桥上弹奏，歌舞，其景诱人。望湖上，红绿掩映间，有“佳人”画船听雨眠，一袭黑衣，曲颈红喙，优雅高贵，不可方物。<img src="/travel/南巡，难寻——扬州/3b981f1310d680ffd100328bdf5e04b.jpg" alt=""><br>​    淮左名都，竹西佳处，烟雨中扬州的瘦西湖，身披轻纱体态轻盈的飞燕，朦胧纤细，仙气四溢，皓腕凝霜，肤若凝脂，流光溢彩，难怪春风十里扬州路，卷上珠帘总不如。祸水红颜也好，身不由己也罢，汉成帝一生有飞燕为伴，夫复何求。</p><p><img src="/travel/南巡，难寻——扬州/244caaed8678c24fc617381fd9b6e14.jpg" alt=""><br>​    烟雨后，西湖瘦，雨稍稍停歇，我坐上小船，摇动船桨，行至湖心。四面竹树环合，花香满溢，扬州小调萦绕耳畔，过钓鱼台，五亭桥，白塔，又是一派皇家园林之相。乘船至南门而出，沿河道步行过何园，至东关街口。</p><p>​    在东关街汉服店中意一身汉服，咬牙买下。行走于古街，甩动魏晋广袖，啜一口桂花酿，提一提深蓝长裙，仿佛置身魏晋时期，我便是那建安七子的曹子建，七步以成诗，见洛神为赋，慨而歌，乐而啸，泼墨为画，曲水流觞，好不自在！<img src="/travel/南巡，难寻——扬州/bb93633a6fc066caf177251d0b96e5b.jpg" alt=""></p><p>​    人言未老莫还乡，我去过的地方，很难再去第二遍，总认为一次就好，一次就尽兴，一次就铭刻于心，深入骨髓，嵌进脑海，留到精神世界去回味。但是扬州，我觉得我还会来第二次，或许会有第三次…</p><p>​    扬州慢我，扬州误我，扬州怡我，人生只合扬州死，禅智山光好墓田！</p><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> travel </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>南巡，难寻——南京续</title>
      <link href="/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E5%8D%97%E4%BA%AC%E7%BB%AD/"/>
      <url>/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E5%8D%97%E4%BA%AC%E7%BB%AD/</url>
      
        <content type="html"><![CDATA[<h2 id="南巡，难寻——南京续"><a href="#南巡，难寻——南京续" class="headerlink" title="南巡，难寻——南京续"></a>南巡，难寻——南京续</h2><a id="more"></a><h5 id="南京理工——二月兰"><a href="#南京理工——二月兰" class="headerlink" title="南京理工——二月兰"></a>南京理工——二月兰</h5><p>4月8日，昨日游玩的疲惫已被今天的新奇所取代，又是元气满满，精神焕发。伸个懒腰，起床洗漱！</p><p>​    扯开窗帘，阳光透过玻璃洒落屋内，惊起一片尘纤。洗漱后，收拾行囊放置前台，在火的带领下，来到南理二月兰。<img src="/travel/南巡，难寻——南京续/微信图片_20190416205847.jpg" alt=""></p><p>​    置身葱绿紫雾之海，我又想起季老的二月兰了，和我眼前的二月兰是否同样呢？是否也无谓悲喜，只是怒放；也是纵浪大化中，一切顺其自然。我俯身向蝴蝶招手，蝴蝶未至，兀自舞动，如雾中仙子，紫气直冲霄汉。于林中穿行，惊起几只白鹭，振动白羽，鸣叫苍穹。</p><h5 id="古鸡鸣寺"><a href="#古鸡鸣寺" class="headerlink" title="古鸡鸣寺"></a>古鸡鸣寺</h5><p>每至一地必看樱。北京玉渊潭，青岛中山公园，武汉等。于是在4月9日下午，樱花大道，一行三人欣然而至。</p><p>​    转角便是樱花大道，我闭眼屏息，身子向右扭转90度，准备好樱花的绝美之景向我扑来。谁知抬眼望去，枝头无一粉色，便只是绿茫茫一片。风卷过，残花起，4月1日樱花大赏的牌子已显得黄旧，花季过，佳期失，独留残花余香，犹自慰藉游人。</p><p>​    不免意兴阑珊，却在绿枝掩映间瞧见一抹黄色，细看时，是一座古刹，门口四个苍劲大字——古鸡鸣寺。<img src="/travel/南巡，难寻——南京续/微信图片_20190416205903.jpg" alt=""></p><p>​    南朝四百八十寺之首，晨钟暮鼓，鸡鸣梵唱。 一路行至香鼎，置身香雾中，按东南西北(我也不知对不对，只知道是顺时针)拜过四方，将香插入鼎中。香气入鼻，不觉已到殿中。抬眼望到大佛宝相庄严，双腿发软，两股战战，膝盖一弯，便跪倒在地。入寺前曾列举想要许下的愿望，此刻却脑海空空，一个也想不起来。放不下的，想得到的，至死不渝的，喜怒哀乐爱欲惧在这一叩尽皆成灰。<br>​    每个人都有这么一片虚无的精神世界，在那世界中的神明，照见五蕴皆空，渡一切苦厄。僧是佛，道是三清，臣是皇，兵是将，你又是谁？</p><h5 id="玄武湖"><a href="#玄武湖" class="headerlink" title="玄武湖"></a>玄武湖</h5><p>从古鸡鸣寺出，越古城墙，是玄武湖。<img src="/travel/南巡，难寻——南京续/微信图片_20190416205857.jpg" alt=""></p><p>​    玄武湖方圆近五里，分作五洲(环洲、樱洲、菱洲、梁洲、翠洲)，洲洲堤桥相通，湖中倒映古城大厦。不知太白重游此地，会否将千余年所作“空余后湖月，波上对江州”拿出撕掉，重绘玄武繁华。</p><p>​    静坐至溪溪下课，吃过火锅，到车站。农历已是烟花三月，下扬州！</p>]]></content>
      
      
      <categories>
          
          <category> travel </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>南巡，难寻——南京</title>
      <link href="/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E5%8D%97%E4%BA%AC/"/>
      <url>/travel/%E5%8D%97%E5%B7%A1%EF%BC%8C%E9%9A%BE%E5%AF%BB%E2%80%94%E2%80%94%E5%8D%97%E4%BA%AC/</url>
      
        <content type="html"><![CDATA[<h2 id="南巡，难寻——南京"><a href="#南巡，难寻——南京" class="headerlink" title="南巡，难寻——南京"></a>南巡，难寻——南京</h2><a id="more"></a><p>​        八天九夜南巡曲终人散，却是一场好聚。江南水乡，草长莺飞，舞榭歌台，眼前尽是南巡春色，心底难寻树俊草香。2019年4月6日周六晚，高铁一路向南疾驰，约2小时后，到达南京南，与虹见面，火火来迎，当晚下榻于南理内酒店。</p><h5 id="南京大屠杀纪念馆"><a href="#南京大屠杀纪念馆" class="headerlink" title="南京大屠杀纪念馆"></a>南京大屠杀纪念馆</h5><p>4月7日晨，吃过学校附近南京汤包与梅干菜锅盔(推荐!)，与火虹来到南京大屠杀纪念馆。</p><p>​        天空灰蓝，青石斑驳，血与泥土铸成砖，国恨在此，殇也在此。入眼便是一位母亲垂着双臂托着她死去的孩子，她披头散发，双眼无神，身心受到痛楚，灵魂四分五裂。绝望恐惧突然萦绕于心，呼吸急促，步履缓慢。4月，南京却已骄阳，炙问人心。<img src="/travel/南巡，难寻——南京/微信图片_20190415203237.jpg" alt=""><br>​        影像难绘地狱之景，血泪难书恶犬之暴，万人坑中层骨堆叠，三十万冤魂长眠此地，历史无言，声若钟磬。<br>​        烈日为烛，金陵作墓，卷起苍穹祭。华夏之地，群狼环伺，岂曰无衣？与子同袍！</p><h5 id="南京大排档"><a href="#南京大排档" class="headerlink" title="南京大排档"></a>南京大排档</h5><p>4月7日午，南京大排档。<img src="/travel/南巡，难寻——南京/微信图片_20190415203301.jpg" alt=""></p><p>​        印象中，南方是甜口的天下，果不其然，烤鸭是甜的，汤包是甜的，还吃了各种糕，团子，圆子，不过，味蕾在欢呼雀跃的感觉让心情也好了许多～鸡汁汤包面紧实滑软，汁甜中似咸，馅香而不腻，才是只应天上有。</p><h5 id="南京总统府"><a href="#南京总统府" class="headerlink" title="南京总统府"></a>南京总统府</h5><p>吃过午饭，行至南京总统府。<img src="/travel/南巡，难寻——南京/微信图片_20190415203313.jpg" alt=""></p><p>​        红花绿树，莺鸣水清，青砖黛瓦，孙文与蒋中正。<br>​        幼年时，我们总喜欢以好坏来区分我们认识的每个人，生活中或电视里，便是非黑即白。但慢慢发现，世界并不只好坏之分，有墨之白，亦有素之黑。总统府院，景色秀丽，办公之地，严肃整洁。睁眼繁花依旧，闭眼枪炮隆隆，也许孙和蒋也是这样吧，处在这个位置，身前草色青青，眼里却是刀光剑影，你死我活！</p><h5 id="秦淮河-夫子庙-老门东-先锋书店"><a href="#秦淮河-夫子庙-老门东-先锋书店" class="headerlink" title="秦淮河 夫子庙 老门东 先锋书店"></a>秦淮河 夫子庙 老门东 先锋书店</h5><p>锦衣夜行，幸好有星辉灯火相伴，黄昏见溪溪于地铁，暗赞。<img src="/travel/南巡，难寻——南京/微信图片_20190415203323.jpg" alt=""><br>​        四人夜游秦淮，河畔夫子庙老门东各种小食。耳畔萦绕戏腔，黛玉伯虎，烟笼寒水，我昔扬帆掠吴楚，尽阅秦淮八艳的风华。青石板路，微风习习，一路吃来，一路聊来，一路风流。先锋书店，闻着熏香，翻开墨页，静坐至夜深，乐及而归。</p>]]></content>
      
      
      <categories>
          
          <category> travel </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SVM</title>
      <link href="/Algorithm/SVM/"/>
      <url>/Algorithm/SVM/</url>
      
        <content type="html"><![CDATA[<h3 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h3><a id="more"></a><ul><li><h5 id="间隔"><a href="#间隔" class="headerlink" title="间隔"></a>间隔</h5><p>在支持向量机中, 我们用间隔刻画划分 超平面与样本之间的距离。</p><p>引理1：$\bf{R}^d$空间中某点$p$到超平面$w^Tx+b=0$的距离为</p></li></ul><script type="math/tex; mode=display">\frac{1}{||w||}|w^Tp+b|</script><p>​    定义1：间隔表示距离划分超平面最近的样本到划分超平面距离的两倍, 即</p><script type="math/tex; mode=display">\gamma=2\min_i\frac{1}{||w||}|w^Tp+b|</script><p>​    定理1：线性支持向量机的目标是找到一组合适的参数 $(w,b)$, 使得</p><script type="math/tex; mode=display">\max_{w,b}\min_i\frac{2}{||w||}|w^Tp+b|</script><script type="math/tex; mode=display">s.t.\ \ \ \ \ \ \ \ \ \ \  y_i(w^Tx_i+b)\gt0,\ \ \ \ \ i=1,2,....m</script><p>​    </p><p>​    即线性支持向量机希望在特征空间找到一个划分超平面, 将属于不同标记的样本分开, 并且该划分超平面距离    各样本最远</p><ul><li><h5 id="线性支持向量机基本型"><a href="#线性支持向量机基本型" class="headerlink" title="线性支持向量机基本型"></a>线性支持向量机基本型</h5><p><strong>引理2</strong>： 若 $(w^<em>,b^</em>) $是定理3优化问题的解, 那么对任意r &gt; 0, $(rw^<em>,rb^</em>) $仍是该优化问题的解</p><p>由于对$(w,b)$的放缩不影响解, 为了简化优化问题, 我们约束$(w,b)$使得</p><script type="math/tex; mode=display">\min_i|w^Tx_i+b|=1</script><p>因此，定理1等价于</p><script type="math/tex; mode=display">\min_{w,b}\frac{1}{2}w^Tw</script><script type="math/tex; mode=display">s.t.\ \ \ \ \ \ \ \ \ \min_iy_i(w^Tx_i+b)=1</script></li><li><h5 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h5><p><strong>引理3</strong>：对偶问题是主问题的下界，即</p><script type="math/tex; mode=display">\max_{\alpha,\beta}\min_uL(u,\alpha,\beta)\le\min_u\max_{\alpha,\beta}L(u,\alpha,\beta)</script></li><li><h5 id="线性支持向量机对偶型"><a href="#线性支持向量机对偶型" class="headerlink" title="线性支持向量机对偶型"></a>线性支持向量机对偶型</h5><p>线性支持向量机的拉格朗日函数为</p><script type="math/tex; mode=display">L(w,b,\alpha)=\frac{1}{2}w^Tw+\sum^m_{i=1}\alpha_i(1-y_i(w^Tx_i+b))</script><p>描述的优化问题是：</p><script type="math/tex; mode=display">\min_{w,b}\max_{\alpha}\frac{1}{2}w^Tw+\sum^m_{i=1}\alpha_i(1-y_i(w^Tx_i+b))</script><script type="math/tex; mode=display">s.t. \ \ \ \ \ \ \ \ \ \alpha_i\ge0,\ \ i=1,2,...,m</script><p>其对偶问题为</p><script type="math/tex; mode=display">\max_{\alpha}\min_{w,b}\frac{1}{2}w^Tw+\sum^m_{i=1}\alpha_i(1-y_i(w^Tx_i+b))........*</script><script type="math/tex; mode=display">s.t. \ \ \ \ \ \ \ \ \ \alpha_i\ge0,\ \ i=1,2,...,m</script><p><strong>定理2</strong>： (线性支持向量机对偶型)线性支持向量机的 对偶问题等价于找到一组合适的参数 $\alpha$, 使得</p><script type="math/tex; mode=display">\min_\alpha\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_jx_i^Tx_j-\sum^m_{i=1}\alpha_i</script><script type="math/tex; mode=display">s.t.\ \ \ \ \ \ \ \ \sum^m_{i=1}\alpha_iy_i=0\\\qquad\qquad\qquad\qquad \alpha_i\ge0,\ \ \ i=1,2,...,m</script><p>证明： 因为公式$*$内层对$(w,b)$的优化属于无约束优化问题, 我们可以通过令偏导等于零的方法得到$(w,b)$的最优值</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w}=0\Rightarrow w=\sum^m_{i=1}\alpha_iy_ix_i\\\frac{\partial L}{\partial b}=0\Rightarrow\sum^m_{i=1}\alpha_iy_i=0</script><p>将其代入公式$*$可得</p></li><li><h5 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h5><ul><li><p>非线性可分问题</p><p>既然在原始的特征空间$\bf{R}^{d}$不是线性可分的, 支持向量机希望通过一个映射 $\phi: \bf{R}^d →\bf{R}^{\tilde{d}}$, 使得数据在新的空间是线性可分的。a</p><p>令$\phi(x)$代表将样本$x$映射到$\bf{R}^{\tilde{d}}$中的特征向量, 参数$w$的维数也要相应变为$\tilde{d}$维。则支持向量机的基本型和对偶型相应变为</p><script type="math/tex; mode=display">\min_{w,b}\frac{1}{2}w^Tw\\\qquad \qquad s.t.\qquad\qquad y_i(w^T\phi(x_i)+b)\ge1,\qquad i=1,2,...,m</script><script type="math/tex; mode=display">\min_\alpha\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_i)-\sum^m_{i=1}\alpha_i\\s.t.\qquad\qquad\sum^m_{i=1}\alpha_iy_i=0,\\\qquad\qquad\qquad\qquad\qquad\alpha_i\ge0,\qquad i=1,2,...m</script></li><li><p>核技巧</p><p>在支持向量机的对偶型中, 被映射到高维的特征向量总是以成对内积的形式存在, 即$\phi(x_i)^T\phi(x_j)$。如果先计算特征在$\bf{R}^{\tilde{d}}$空间的映射, 再计算内积，会非常的复杂。</p><p>核技巧旨在将特征映射和内积这两步运算压缩为一步, 构造一个核函数 $\kappa(x_i,x_j)$, 使得</p><script type="math/tex; mode=display">\kappa(x_i,x_j)=\phi(x_i)^T\phi(x_j)</script><p>几种常用核函数：</p><p>| 名称     | 形式                                   | 优点                                            | 缺点                                |<br>| ———— | ——————————————————— | ———————————————————————- | —————————————————- |<br>| 线性核   | $x_i^Tx_j$                             | 有高效实现，不易过拟合                          | 无法解决非线性可分问题              |<br>| 多项式核 | $(\beta x_i^Tx_i+\theta)^n$            | 比线性核更一般，$n$直接描述了被映射空间的复杂度 | 参数多，当$n$很大时会导致计算不稳定 |<br>| RBF核    | $\exp(-\frac{||x_i-x_j||}{2\alpha^2})$ | 只有一个参数，没有计算不稳定问题                | 计算慢，过拟合风险大                |</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>logistic regression</title>
      <link href="/Algorithm/logistic-regression/"/>
      <url>/Algorithm/logistic-regression/</url>
      
        <content type="html"><![CDATA[<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><a id="more"></a><p>逻辑回归不同于线性回归，虽然带有回归二字，其本质做的是分类的活。</p><ul><li><strong>sigmoid函数</strong></li></ul><p>sigmoid函数是很常见的一种激活函数，由于其形状像“S”型，又称其为“S”型激活函数。其公式为</p><script type="math/tex; mode=display">g(x) = \frac{1}{1+e^{-x}}</script><p>函数图像为：</p><p><img src="/Algorithm/logistic-regression/sigmoid.jpg" alt="sigmoid"></p><p>将输出限制在0-1之间，大于等于0.5判为1，小于0.5判为0</p><ul><li><strong>代价函数和梯度下降</strong></li></ul><p>我们把期望函数定义为：</p><script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>来表示我们预测的输出，那么，有：</p><script type="math/tex; mode=display">P(y=1|x;\theta)=h_{\theta}(x)</script><script type="math/tex; mode=display">P(y=0|x;\theta)=1-h_{\theta}(x)</script><p>可以得到：</p><script type="math/tex; mode=display">P(y|x;\theta)=(h_{\theta}(x))^y(1-h_{\theta}(x))^{1-y}</script><p>最大似然函数为：</p><script type="math/tex; mode=display">L(\theta)=Max\prod^m_{i=1}(h_{\theta}(x))^{y_i}(1-h_{\theta}(x))^{1-y_i}</script><p>化为对数似然：</p><script type="math/tex; mode=display">l(\theta)=\log L(\theta)=Max\sum^m_{i=1}(y_i\log(h_{\theta}))+(1-y_i)\log(1-h_{\theta}(x_i))</script><p>代价函数为：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}\log(h_{\theta}(x^{(i)}))-(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))]</script><p>梯度为：</p><script type="math/tex; mode=display">\nabla J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y)x^{(i)}_j</script><p>仍可使用梯度下降法去更新参数$\theta$</p><h3 id="正则化项"><a href="#正则化项" class="headerlink" title="正则化项"></a>正则化项</h3><p>正则化是为了消除过拟合而采用的一种方式，将逻辑回归的损失函数加上正则化项，表示为：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}\log(h_{\theta}(x^{(i)}))-(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta^2_j</script><p>梯度为：</p><p>对于$j=0$有</p><script type="math/tex; mode=display">\nabla J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y)x^{(i)}_j</script><p>对于$j\ge1$有</p><script type="math/tex; mode=display">\nabla J(\theta)=(\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y)x^{(i)}_j)+\frac{\lambda}{m}\theta_j</script>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>建立github博客</title>
      <link href="/learning-note/%E5%BB%BA%E7%AB%8Bgithub%E5%8D%9A%E5%AE%A2/"/>
      <url>/learning-note/%E5%BB%BA%E7%AB%8Bgithub%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h3 id="搞github博客从头至尾"><a href="#搞github博客从头至尾" class="headerlink" title="搞github博客从头至尾"></a>搞github博客从头至尾</h3><a id="more"></a><ul><li><h4 id="建github-io"><a href="#建github-io" class="headerlink" title="建github.io"></a>建github.io</h4></li></ul><hr><p>首先需要申请自己的github账号，在自己的首页点击<strong>Repositories</strong>,然后点击<strong>new</strong>，如图所示</p><p><img src="/learning-note/建立github博客/1546954199271.jpg" alt=""></p><p>然后在name上填写用户名.github.io，接着Create（因为我创建过此仓库，故不可重复）</p><p><img src="/learning-note/建立github博客/1546954336467.jpg" alt=""></p><p>这个生成好的 Repository 就是用来存放博客内容的地方，也只有这个仓库里的内容，才会被 mycaozf.github.io 这个网页显示出来</p><ul><li><h4 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h4></li></ul><hr><p>Hexo 是一个博客框架。它把本地文件里的信息生成一个网页。</p><p>使用 Hexo 之前，需要先安装 Node.js 和 Git。</p><h5 id="操作如下"><a href="#操作如下" class="headerlink" title="操作如下"></a>操作如下</h5><ol><li><p>安装 Node.js</p><ul><li><p>前往 <a href="https://nodejs.org/en/" target="_blank" rel="noopener">https://nodejs.org/en/</a></p></li><li><p>点击 10.15.0 LTS 下载</p></li><li><p>安装</p></li><li><p>打开 cmd， 输入 <code>node -v</code></p></li><li><p>得到：v10.15.0</p><p>安装成功</p></li></ul></li><li><p>安装 Git</p><ul><li><p>前往 <a href="https://git-scm.com/" target="_blank" rel="noopener">https://git-scm.com/</a></p></li><li><p>点击 Downloads</p></li><li><p>打开 cmd， 输入 <code>git --version</code></p></li><li><p>得到：git version 2.20.1.windows</p><p>安装成功</p></li></ul></li><li><p>安装 Hexo</p><ul><li><p>打开 cmd</p></li><li><p>输入 <code>npm install -g hexo-cli</code></p></li><li><p>回车开始安装</p></li><li><p>输入 <code>hexo -v</code></p><p>安装成功</p></li></ul></li><li><p>创建本地博客</p><ul><li><p>在D盘下创建文件夹 blog</p></li><li><p>鼠标右键 blog，选择 Git Bash Here。 如果没有安装 Git，就不会有这个选项。</p></li><li><p>Git Bash 打开之后，所在的位置就是 blog 这个文件夹的位置。（/d/blog）</p></li><li><p>输入 <code>hexo init</code> 将 blog 文件夹初始化成一个博客文件夹。</p></li><li><p>输入 <code>npm install</code> 安装依赖包。</p></li><li><p>输入 <code>hexo g</code> 生成（generate）网页。 由于我们还没创建任何博客，生成的网页会展示 Hexo 里面自带了一个 Hello World 的博客。</p></li><li><p>输入 <code>hexo s</code> 将生成的网页放在了本地服务器（server）。</p></li><li><p>浏览器里输入 <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> 。 就可以看到刚才的成果了。</p></li><li><p>回到 Git Bash，按 Ctrl+C 结束。</p><p>此时再看 <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> 就是无法访问了。</p></li></ul></li><li><p>发布一篇博客</p><ul><li>继续在 Git Bash 里，所在路径还是 /d/blog。输入 <code>hexo new &quot;My First Post&quot;</code></li><li>在 D:\blog\source_posts 路径下，会有一个 My-First-Post.md 的文件。 编辑这个文件，然后保存。</li><li>回到 Git Bash，输入 <code>hexo g</code></li><li>输入 <code>hexo s</code></li><li>前往 <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> 查看成果。</li><li>回到 Git Bash，按 Ctrl+C 结束</li></ul></li></ol><ul><li><h4 id="将本地-Hexo-博客部署在-Github-上"><a href="#将本地-Hexo-博客部署在-Github-上" class="headerlink" title="将本地 Hexo 博客部署在 Github 上"></a>将本地 Hexo 博客部署在 Github 上</h4><hr><h5 id="操作如下："><a href="#操作如下：" class="headerlink" title="操作如下："></a>操作如下：</h5><ol><li><p>获取 Github 对应的 Repository 的链接。</p><ul><li><p>登陆 Github，进入到 mycaozf.github.io</p></li><li><p>点击 Clone or download</p></li><li><p>复制 URL 待用</p><p>我的是 <code>https://github.com/MyCaoZF/mycaozf.github.io.git</code></p></li></ul></li><li><p>修改博客的配置文件</p><ul><li><p>打开配置文件 /d/blog/_config.yml </p></li><li><p>找到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#Deployment</span><br></pre></td></tr></table></figure><p>填入以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:  </span><br><span class="line">  type: git  </span><br><span class="line">  repository: https://github.com/MyCaoZF/mycaozf.github.io.git  </span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></li></ul></li><li><p>部署</p><ul><li><p>回到 Git Bash</p></li><li><p>输入 <code>npm install hexo-deployer-git --save</code> 安装 hexo-deployer-git 此步骤只需要做一次。</p></li><li><p>输入 <code>hexo d</code></p></li><li><p>得到 <code>INFO Deploy done: git</code> 即为部署成功</p><p>之前我们创建的 ReadMe.md 会被自动覆盖掉。</p></li></ul></li><li><p>查看成果</p><p>前往 mycaozf.github.io 即可。</p></li></ol></li><li><h4 id="使用-Next-主题"><a href="#使用-Next-主题" class="headerlink" title="使用 Next 主题"></a>使用 Next 主题</h4></li></ul><hr><p><a href="https://hexo.io/themes/" target="_blank" rel="noopener">更多 Hexo 的主题看这里</a></p><h5 id="操作如下：-1"><a href="#操作如下：-1" class="headerlink" title="操作如下："></a>操作如下：</h5><ol><li><p>回到 Git Bash。 输入 <code>git clone https://github.com/iissnan/hexo-theme-next themes/next</code></p><p>这样，该主题的文件就全部克隆到 D:\blog\themes\next 下面。</p></li><li><p>修改博客配置文件</p><ul><li><p>打开 D:\blog_config.yml</p></li><li><p>找到 <code>theme:</code></p></li><li><p>把 Hexo 默认的 lanscape 修改成 next。 即 <code>theme: next</code></p></li><li><p>找到 <code># Site</code>，添加博客名称，作者名字等。</p></li><li><p>在 <code>language</code> 后面填入 en 或者 zh-Hans，选择英文或者中文。</p></li><li><p>找到 <code># URL</code>, 填入 url。比如 <code>url: https://mycaozf.github.io</code></p><p>填入名字后会有很风骚的 © 2017 CaoZF 的字样出现在博客底部。</p></li></ul></li><li><p>重新生成部署即可</p><ul><li><p>回到 Git Bash。输入 <code>hexo g -d</code>就可以了。</p><p>先把修改的内容生成网页，再部署。</p></li></ul></li><li><p>查看成果</p><p>前往 mycaozf.github.io 即可。</p></li></ol><ul><li><h4 id="在Hexo中使用LaTeX语法输入数学公式"><a href="#在Hexo中使用LaTeX语法输入数学公式" class="headerlink" title="在Hexo中使用LaTeX语法输入数学公式"></a>在Hexo中使用LaTeX语法输入数学公式</h4></li></ul><hr><h5 id="在Hexo中启用MathJax"><a href="#在Hexo中启用MathJax" class="headerlink" title="在Hexo中启用MathJax"></a>在Hexo中启用MathJax</h5><p>此处以Next主题其为例，在Next中启用MathJax只需要一步。在<code>d/blog/theme/next</code>下找到<code>_config.yml</code>。用文本编辑器找到如下部分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plain _config.yml# MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: false</span><br><span class="line">  per_page: true</span><br><span class="line">  cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure><p>将第二行修改为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">enable: true</span><br></pre></td></tr></table></figure><p>即可。</p><p>如此以来就在Next主题中启用了MathJax支持，但事实上还存在着不少问题：</p><ul><li>部分数学公式直接显示为代码的形式</li><li>部分数学公式的下标、角标等显示异常</li><li>希腊字母字体显示异常</li></ul><h5 id="更换渲染引擎"><a href="#更换渲染引擎" class="headerlink" title="更换渲染引擎"></a>更换渲染引擎</h5><p>Hexo默认使用<code>hexo-renderer-marked</code>引擎进行网页渲染，其中对许多字符诸如划线、下划线、中括号等定义了转义。因此，在进行网页渲染时，数学公式中的这些字符先通过<code>hexo-renderer-marked</code>进行转义，就发生了歧义，而再通过MathJax渲染出来的数学公式，自然就显示不正常了。<br>在知道了原因以后，问题也就迎刃而解了，解决方法就是更换Hexo默认的<code>hexo-renderer-marked</code>渲染引擎。<code>hexo-renderer-kramed</code>就是一个不错的选择，它在<code>hexo-renderer-marked</code>的基础上修复了一些Bug，其中就包括取消大部分多余的转义。</p><p>卸载<code>hexo-renderer-marked</code>，安装<code>hexo-renderer-kramed</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-kramed --save`</span><br></pre></td></tr></table></figure><h5 id="确保所有公式都被渲染"><a href="#确保所有公式都被渲染" class="headerlink" title="确保所有公式都被渲染"></a>确保所有公式都被渲染</h5><p>在条件支持的情况下，MathJax会对页面上的所有位置的标签内都进行渲染；但在条件不允许时，MathJax对数学公式的渲染可能止于页面上的某个位置，其后的数学公式都将不被渲染而直接显示为代码的形式。<br>这时，就需要在文章的开头font-matter中手动打开MathJax的开关，如下图：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">title: index.html</span><br><span class="line">date: 2016-12-28 21:01:30</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><p>如上，在使用了数学公式的页面上增加一行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><p>这样就能确保页面上所有数学公式都被正确渲染了。</p><p>但是以上方法还是不能转换行内公式，即一行内不能够有两对dollar符号，具体解决方法正在试，不过已不影响正常使用！</p><p><strong>参考文献</strong>：</p><p><a href="https://www.once4623.site/2017/10/03/2017-10-04--Use-MathJax-In-Hexo-Next/" target="_blank" rel="noopener">在Hexo中使用LaTeX语法输入数学公式</a></p><p><a href="https://ryanluoxu.github.io/2017/11/24/%E7%94%A8-Hexo-%E5%92%8C-GitHub-Pages-%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">用 Hexo 和 GitHub Pages 搭建博客</a></p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      
      
      <categories>
          
          <category> learning-note </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>BP</title>
      <link href="/Algorithm/BP/"/>
      <url>/Algorithm/BP/</url>
      
        <content type="html"><![CDATA[<h3 id="反向传播-BP-算法"><a href="#反向传播-BP-算法" class="headerlink" title="反向传播(BP)算法"></a>反向传播(BP)算法</h3><a id="more"></a><h4 id="参数设定"><a href="#参数设定" class="headerlink" title="参数设定"></a>参数设定</h4><p>$w_{jk}^l$表示第$l-1$层的第$k$个神经元连接到$l$层第$j$个神经元的权重</p><p>$b_j^l$表示第$l$层的第$j$个神经元的偏置</p><p>$z_j^l$表示第$l$层的第$j$个神经元的输入</p><script type="math/tex; mode=display">z_j^l=\sum_kw_{jk}^la_k^{l-1}+b_j^l</script><p>$a_j^l$表示第$l$层的第$j$个神经元的输出</p><script type="math/tex; mode=display">a_j^l=\sigma(z_j^l)=\sigma(\sum_kw_{jk}^la_k^{l-1}+b_j^l)</script><p>其中$\sigma$表示激活函数</p><p>$\delta_j^l$表示第$l$层第$j$个神经元中产生的错误</p><p>定义为$\delta_j^l=\frac{\partial J}{\partial z_j^l}$    </p><p>其中，有$\delta_j^l=\frac{\partial J}{\partial z_j^l}=\frac{\partial J}{\partial a_j^l}\cdot\frac{\partial a_j^l}{\partial z_j^l}$                                      </p><p>用矩阵表示为$\frac{\partial J}{\partial a^L}\odot\frac{\partial a^L}{\partial z^L}=\nabla_aJ\odot\sigma’(z^L)$     ($\odot$表示Hadamard乘积，用于矩阵或向量间点对点乘法运算)</p><p>又有</p><script type="math/tex; mode=display">\delta_j^l=\frac{\partial J}{\partial z_j^l}=\sum_k\frac{\partial J}{\partial z_k^{l+1}}\cdot\frac{\partial  z_k^{l+1}}{\partial a_j^l}\cdot\frac{\partial a_j^l}{\partial z_k^l}=\sum_k\delta_k^{l+1}\cdot\frac{\partial (w_{kj}^{l+1}a_j^l+b_k^{l+1})}{\partial a_j^l}\cdot\sigma'(z_j^l)=\sum_k\delta_k^{l+1}\cdot w_{kj}^{l+1}\cdot\sigma'(z_j^l)</script><p>用矩阵表示为$\delta^l = ((w^{l+1})^T\delta^{l+1})\odot\sigma’(z^l)$</p><h4 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h4><p>计算权重的梯度：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial w_{jk}^l}=\frac{\partial J}{\partial z_k^l}\cdot\frac{\partial z_j^l}{\partial w_{jk}^l}=\delta_j^l\cdot\frac{\partial (w_{kj}^la_k^{l-1}+b_j^l)}{\partial w_{jk}^{l}}=a_k^{l-1}\delta_j^l</script><p>计算偏置的梯度：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial b_j^l}=....=\delta_j^l</script><h4 id="使用梯度下降法"><a href="#使用梯度下降法" class="headerlink" title="使用梯度下降法"></a>使用梯度下降法</h4><p>使用梯度下降训练参数：</p><script type="math/tex; mode=display">w_{jk}^l = w_{jk}^l-\frac{\eta}{m}\delta_j^la_k^{l-1}</script><script type="math/tex; mode=display">b_j^l = b_j^l-\frac{\eta}{m}\delta_j^l</script>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>gradient descent</title>
      <link href="/Algorithm/gradient-descent/"/>
      <url>/Algorithm/gradient-descent/</url>
      
        <content type="html"><![CDATA[<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><a id="more"></a><h5 id="梯度："><a href="#梯度：" class="headerlink" title="梯度："></a>梯度：</h5><ul><li>在单变量的函数中，梯度就是函数的微分，代表着函数在某个定点切线的斜率。</li><li>在多变量函数中，梯度是一个向量，梯度的方向就指出了函数在给定点上升最快的方向</li></ul><h5 id="参数更新公式："><a href="#参数更新公式：" class="headerlink" title="参数更新公式："></a>参数更新公式：</h5><script type="math/tex; mode=display">\theta^1 = \theta^0 - \alpha\nabla J(\theta)</script><p>​    其中$\alpha$为步长，$\nabla J(\theta)$为损失函数的梯度，$\theta$为权重</p><h5 id="代价函数："><a href="#代价函数：" class="headerlink" title="代价函数："></a>代价函数：</h5><p>​    衡量模型预测的值$h_{\theta}(x^{(i)})$与真实值$y$之间的差异的函数</p><p>形式：</p><ul><li>均方误差<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y)^2</script>  $m$为训练样本的个数<script type="math/tex; mode=display">\nabla J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y)x^{(i)}</script></li></ul><ul><li><p>交叉熵(用于逻辑回归)</p><script type="math/tex; mode=display">J(\theta) = -\frac{1}{m}[\sum_{i=1}^{m}(y^{(i)}\log h_{\theta}(x^{(i)})+(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))]</script><script type="math/tex; mode=display">\nabla J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y)x^{(i)}</script><p>​    其中</p><script type="math/tex; mode=display">h_{\theta}(x^{(i)}) = \frac{1}{1+e^{-\theta x}}</script></li></ul><h5 id="用矩阵表示：-均方误差"><a href="#用矩阵表示：-均方误差" class="headerlink" title="用矩阵表示：(均方误差)"></a>用矩阵表示：(均方误差)</h5><script type="math/tex; mode=display">        J(\theta) = \frac{1}{2m}(X\theta-y)^T(X\theta-y)</script><script type="math/tex; mode=display">\nabla J(\theta)=\frac{1}{m}X^T(X\theta-y)</script>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>game theory</title>
      <link href="/read/game-theory/"/>
      <url>/read/game-theory/</url>
      
        <content type="html"><![CDATA[<h2 id="博弈论"><a href="#博弈论" class="headerlink" title="博弈论"></a>博弈论</h2><a id="more"></a><h3 id="一个终极目标"><a href="#一个终极目标" class="headerlink" title="一个终极目标"></a>一个终极目标</h3><p>博弈的终极目标，是达到纳什均衡，纳什均衡是谋略计算的终点。</p><h3 id="七个重要概念"><a href="#七个重要概念" class="headerlink" title="七个重要概念"></a>七个重要概念</h3><p><strong>【纳什均衡】</strong>如果博弈各方都是足够聪明的人，大家最终的策略选择一定是这么一个局面：在这个局面里大家都认命了，谁也无法单方面改变策略去谋求一个对自己更好的结局。 如果一个现象能够在社会中长期稳定地存在，它对参与的各方来说就一定是个纳什均衡。纳什均衡告诉我们评价一个局面不能只看它是不是对整体最好，它必须得让每个参与者都不愿意单方面改变才行。<br><strong>【帕累托最优】</strong>帕累托是一位意大利经济学家，帕累托改进的意思是这个改进能在不伤害任何一个人利益的同时，使得至少一个人的境遇变得更好。如果一个局面已经好到没有帕累托改进的余地了，这个局面就叫“帕累托最优”。<br><strong>【压倒性策略】</strong>（Dominant Strategy）。在博弈局面中，你有一个策略压倒其他一切策略，不管对手怎么做，这个策略对你来说都是最好的。反之，“被压倒性策略（Dominated Strategy）”，就是不管别人怎么做，你这么做对你都是不好的。<br><strong>【囚徒困境】</strong>合作则两利、背叛则两伤。经济学中所谓“负的外部性”、“公地悲剧”、价格战，国际政治中的军备竞赛，动物世界中的互助行为，体育比赛中的使用禁药，医学中的抗生素滥用，包括心理学中的上瘾现象，其实都是囚徒困境。<br><strong>【博弈演化论】</strong>专门研究策略的流行规律的学问。博弈的演化，是策略的竞争。如果使用一个策略能带来好的报偿，人们就会模仿这个策略，这个策略就会流行开来。<br><strong>【KMRW定理】</strong>用四个经济学家名字的首字母命名。在不完全信息博弈中，参与者不知道对方是好人还是理性人，那么只要博弈重复的次数足够多，合作能带来足够的好处，双方都会愿意维护自己是好人的这样一个声誉，前期尽可能地保持合作，到最后才选择背叛。<br><strong>【Player】</strong>在游戏中叫玩家，在体育比赛中叫选手，在博弈论中叫参与者 —— 其实都是一个意思，博弈论（Game Theory）说的都是 game。有一点参与游戏的精神，你就有权在规则范围内采取对自己最有利的行动，你就是积极主动的，你就会平等对待对手 —— 你就既不是一个浑浑噩噩整天根据别人设定做事的人，也不会有整个世界绕着自己转的幻觉。</p><h3 id="六个博弈局面"><a href="#六个博弈局面" class="headerlink" title="六个博弈局面"></a>六个博弈局面</h3><p><strong>博弈局面一：</strong>各方有强烈的合作意愿，而博弈有不止一个纳什均衡。<br><strong>采取策略：</strong>找到“聚焦点”</p><ol><li>一个博弈中会有多个纳什均衡。比如交通规则中“右侧通行”和“左侧通行”，都是纳什均衡。</li><li>聚焦点就是在众多可能的纳什均衡中最显眼的那一个，人们会自动在这一点上达成合作。聚焦点的作用是协调。</li><li>聚焦点举例：度量衡、键盘、限速牌、指导价、平均年薪……可以是生活习惯，可以是历史传承，可以是传统文化，可以是先下手为强，可以是政府指导，可以是随便找到的什么借口，实在不行还可以抽签。</li></ol><p><strong>博弈局面二：</strong>囚徒困境——合作对所有人都有好处，但背叛对背叛者有直接的好处。<br><strong>采取策略：</strong>如果博弈是可重复的，应该寻求对背叛者进行惩罚。防止背叛，最直观的办法就是把单次博弈变成重复博弈。重复博弈之所以有效，是因为背叛者会受到惩罚。如果博弈是可重复的，应该寻求对背叛者进行惩罚。以牙还牙是最经典的做法，但适当的宽容更能促成合作。</p><ol><li>有效的惩罚必须得满足3个条件：你得能发现背叛行为；惩罚必须得是可信的，对方知道他一定会受到惩罚；惩罚的力度得足够。</li><li>“以牙还牙”是个保守的策略：1）不管跟谁，第一轮我都选择合作；2）第一轮过后，我就复制对手上一轮的做法。你上一轮要是跟我合作，我下一轮也跟你合作。你要是背叛了我，我下一轮也背叛你。如果你在哪一轮又选择合作了，那我还继续和你合作。我合作，我报复，我原谅，我只是模仿你上一轮的动作。</li><li>“以牙还牙”其实是个脆弱的策略：对错误不够友好，它不够宽容。改进版的以牙还牙策略是：对方背叛我一次，我继续合作；只有当对方连续背叛我两次，我再报复。</li></ol><p><strong>博弈局面三：</strong>参加博弈的人数比较少，合作的利益比较大，各方就会形成串通和合谋，尽管这么做不一定对社会有好处。</p><ol><li>戴比尔斯公司的钻石垄断、美国商店的价格匹配、商家之间通过比价网站价格协调、包括大型募捐活动，都是这样的局面。</li><li>合作的利益大就不会竞争，背叛的成本低才会背叛。</li><li>打破这个局面的一个办法就是扩大市场准入，让更多的参与者进来，让商家的协调没那么容易。另外一个办法就是依靠政府的力量反垄断，相当于全体消费者联合起来去对付那些巨头。</li></ol><p><strong>博弈局面四：</strong>信息不对称<br><strong>采取策略：</strong>传达信息最好的办法是发信号，这意味着你要用行动去证明自己。</p><ol><li>一种常见的博弈局面是有一方参与者知道一个关键信息，而另外一方不知道。一方强烈地想让另一方知道他的信息，但是又怕对方不信。一方强烈地想知道对方的信息，但是又怕对方说谎。这就叫“信息不对称”。</li><li>花钱、花时间、或者花的是脸面，但又都没什么直接的用处——在博弈论看来，人们做这样的事情，都是为了解决信息不对称。</li></ol><p><strong>博弈局面五：</strong>最高级的应用：设计博弈</p><ol><li>学习博弈论的确有一个比做 player 更高级的视角。那就是做为规则的制定者，去给人设计博弈局面。</li><li>房产经纪人薪酬设计、竞拍规则设计，都可以改变局面。但用于真实世界的制度设计，它未必有实用价值。</li><li>一般人遵守规则，少数人违反规则，有的人制定规则。设计一个博弈，比参加一个博弈要难得多，这是管理者的学问。</li></ol><p><strong>博弈局面六：</strong>纳什均衡是博弈的结局，可是真实世界从来都没有结局 —— 这是因为博弈局面总在变化，我们甚至可以主动改变博弈。</p><h3 id="六个博弈策略"><a href="#六个博弈策略" class="headerlink" title="六个博弈策略"></a>六个博弈策略</h3><p><strong>博弈策略一：</strong>（装）做好人</p><ol><li>在残酷世界里选择做好人表面上看是非理性的 —— 但KMRW定理告诉我们，只要博弈有比较多、哪怕只是<em>有限次的重复</em>，做好人其实是有利的。</li><li>但博弈论专家绝对不会建议你去做真正的好人。好人经常对世界有一厢情愿的期待。博弈论专家会说这种想法非常危险。事实上，如果你身处一个比较险恶的社会环境，那你不但不应该做好人，而且应该装坏人。</li><li>但好人跟好人之间形成了一个想象的共同体。这其实是一个幻觉，但是没办法，想象的共同体是最强大的社会力量。这其实也是理性的。</li></ol><p><strong>博弈策略二：寻求监管</strong></p><ol><li>一个破解囚徒困境的直观解决方案：让第三方监管。</li><li>监管的本质是改变了博弈的报偿（payoff）。有了有效的监管，不合作就不但没有好处，而且还会受到惩罚，不合作的行为自然就会大大减少。</li><li>比自己管、私有化和政府管这三种监管方法更新颖的是让博弈各方之外的“第四方”监管。更高级的监管是监管者和被监管者的合作。</li><li>你应该把政府也当做一个 player。而且政府也应该把自己视为一个 player。既然是参加博弈的 player，政府也需要博弈论。</li></ol><p><strong>博弈策略三：</strong>先下手为强＋后发者优势</p><ol><li>动态博弈</li><li>小鸡博弈：只要你能确定对手的底线，那么先发制人，造成既成事实，就能逼迫对手就范。</li><li>悬崖策略是动态进行的小鸡游戏。双方每一步都在推动危机升级，这是一个危险的边缘游戏。</li><li>最好的办法给对方一个威慑，让他根本不敢出手。威慑有三个要素：实力、决心和让对手知道。</li><li>先发者暴露信息，后发者利用信息。</li><li>后发优势 = 先发者的信息 + 后发者的出手权。信息是模仿机会，出手权是创新机会。</li></ol><p><strong>博弈策略四：</strong>威胁和承诺</p><ol><li>威胁和承诺都是在博弈双方都没有采取实质性行动之前，一方通知另一方的声明。所谓威胁，就是我要求你不要去做某件事 —— 我说如果你做了，我就会对你进行惩罚。所谓承诺，就是我要求你去做某件事 —— 如果你做了，我就会给你一个奖励。</li><li>只有可信的威胁和承诺才有意义。</li><li>可信 = 别无选择。发出可信的威胁或者承诺有三个办法：给别人惩罚你的权力；主动取消自己的选项；建立声望。</li></ol><p><strong>博弈策略五：</strong>随机选择策略</p><ol><li>只说谎话就等于只说实话。想要真的迷惑对手，你必须把谎话和实话混合起来。</li><li>你的混合概率选择，应该把对手能得到的最大报偿给最小化。你要按照一定的概率，混合自己的打法。你混合打法的这个规律，必须是让对手无法利用的。</li><li>不是真随机，就会被破解。随机性，才是真正的“诡道”。</li></ol><p><strong>博弈策略六：</strong>最高视角：观察不同博弈策略在人群中的演化。博弈永无休止。 </p><ol><li>即便纳什均衡并不只有一种，冥冥之中仍然存在着一些规律，在限制我们选择策略的自由。这些规律决定了社会的演化。</li><li>如果使用一个策略能带来好的报偿，人们就会模仿这个策略，这个策略就会流行开来。</li><li>策略的优劣不是永恒的。你必须考虑当前社会的博弈格局，特别是其他人都在使用什么策略，才知道自己的最佳策略是什么。到底要在什么比例的情况下随大流，甚至要不要随大流，都取决于具体的博弈格局。</li></ol><p><strong>若干金句</strong><br>＊计谋要是太多，愚蠢的人就不够用了。博弈论研究的是理性人之间的博弈。<br>＊研究博弈论就好像下棋一样，你要考虑你的每一个行动都是有后果的，你要事先想好对方会有什么反应，然后你再怎么应对，然后对方再反应……一直到最后是个什么结果。<br>＊理想青年喜欢帕累托最优，理性青年寻找纳什均衡。<br>＊博弈论是人类理性行为的第一性原理。<br>＊有一种困境叫自由，有一种解放叫禁止。<br>＊我们年轻时候的雄心壮志变成了对社会的低头，我们感慨世风日下人心不古，我们嘱咐子女不要锋芒毕露，可我们又暗自期望他们能走一条少有人走的路。一切都仿佛是个性和现实之间的对抗，殊不知一切的背后……都是数学。<br>＊所有人都意识不到博弈的时候，可能你诗情画意都能赢。少数人意识到博弈的时候，谁意识到博弈谁赢。大家都意识到博弈了，那就只能比执行力 —— 或者看谁能意识到新的博弈。<br>＊一个合格的 player，应该拥有四个作风 —— 有限、务实、慎重、客观。</p>]]></content>
      
      
      <categories>
          
          <category> read </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
